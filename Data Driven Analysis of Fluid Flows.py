# -*- coding: utf-8 -*-
"""FM_Project_b22me004_b22me072_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13FlKuUHOpRPIAf5JG6YdNyNSF3qkgwP2
"""

import cv2
import os
import shutil
import numpy as np
import matplotlib.pyplot as plt

"""**Question 3**"""

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import matplotlib.pyplot as plt

def frames(video_path, folder, interval_seconds, color_scale):
    if os.path.exists(folder):
    # If the folder exists, delete it and its contents
      shutil.rmtree(folder)

    cap = cv2.VideoCapture(video_path)
    os.makedirs(folder, exist_ok=True)

    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(frame_rate * interval_seconds)
    frame_count = 0
    saved_frame_count = 0
    plt.figure(figsize=(15, 4))

    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == False:
            break
        frame_count += 1
        if frame_count % frame_interval == 0:
            saved_frame_count += 1
            frame_filename = os.path.join(folder, f"frame_{saved_frame_count:04d}.jpg")
            cv2.imwrite(frame_filename, frame)
            if color_scale == 'RGB':
                plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            elif color_scale == 'Grayscale':
                plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), cmap='gray')
            plt.title(f"Frame {saved_frame_count} - {color_scale}")
            plt.axis("off")
            plt.show()

    cap.release()

    print(f"Total frames extracted: {saved_frame_count}")

    return saved_frame_count

# Example usage:
video_path = "/content/drive/MyDrive/3CYL_Flow11.mp4"
output_folder = "TimeIntervalFrames"
frames(video_path, output_folder, interval_seconds=0.5, color_scale='RGB')

def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        if os.path.isfile(img_path):
            img = cv2.imread(img_path)
            if img is not None:
                images.append(img)
    return images

def display_images_sizes(images):
    for i, img in enumerate(images):
        print(f"Image {i+1}: Rows = {img.shape[0]}, Columns = {img.shape[1]} , No. of pixels = {img.shape[0]*img.shape[1]}" )

# Path to the folder containing images
folder_path = "TimeIntervalFrames"  # Update with the actual folder path

# Load images from the folder
images = load_images_from_folder(folder_path)

# Display sizes of the loaded images
display_images_sizes(images)

import os
import cv2

# Update with the actual folder path
folder_path = "TimeIntervalFrames"

# List to store the downsampled grayscale images
downsampled_images_gray = []

# Specify the new dimensions for downsampling
new_width = 1558  # New width of the image
new_height = 708  # New height of the image

# Loop through the files in the folder
for filename in os.listdir(folder_path):
    img_path = os.path.join(folder_path, filename)
    if os.path.isfile(img_path):
        # Read the image in grayscale
        img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img_gray is not None:
            # Downsample the image to the new dimensions
            downsampled_img_gray = cv2.resize(img_gray, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
            # Append the downsampled grayscale image to the list
            downsampled_images_gray.append(downsampled_img_gray)

# Display shape of each downsampled image
for img_gray in downsampled_images_gray:
    print("Downsampled Image shape:", img_gray.shape)

plt.imshow(downsampled_images_gray[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

mean_image = np.mean(downsampled_images_gray, axis=0)
plt.imshow(mean_image)
plt.title('Mean Image')
plt.axis('off')
plt.show()
mean_image.shape

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

# Stack images into a matrix
stacked_images = stack_images(downsampled_images_gray)
print(stacked_images)
stacked_images.shape

# Compute mean image
mean_image11 = np.mean(stacked_images, axis=0)
print(mean_image11)
mean_image11.shape

image_matrix = np.stack(downsampled_images_gray)
print("Shape of image matrix:", image_matrix.shape)

# Step 2: Subtract the mean from each image
centered_images = stacked_images - mean_image11
print(centered_images)
print(centered_images.shape)

data_matrix=centered_images

# Perform Singular Value Decomposition (SVD)
U, S, Vt = np.linalg.svd(data_matrix, full_matrices=False)
print(U)
print("spatial modes ",U.shape)
print(S)
print("singular values ",S.shape)
print(Vt)
print("temporal coefficients ",Vt.shape)

# Select the top 10 modes
top_modes = Vt[:10, :]

print(top_modes)
top_modes.shape

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), S, linestyle='-', color="red")
plt.title('Singular Values Distribution')
plt.xlabel('Index')
plt.ylabel('Singular Value')
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate energy contribution of each mode
energy_contributions = (S ** 2) / total_energy

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
# plt.grid(True)
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes = np.sum(S[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage = (total_energy_top_10_modes / total_energy) * 100

print("Total contribution of the top 10 modes as a percentage of total energy:", total_contribution_top_10_modes_percentage,"%")

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
# Calculate cumulative variance
cumulative_variance = np.cumsum(S ** 2) / np.sum(S ** 2)

num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

print(f"Number of modes to retain {required_variance * 100}% variance: {num_modes_90_percent}")

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), cumulative_variance, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent, cumulative_variance[num_modes_90_percent - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent, color='blue', linestyle='--')
plt.show()

# Number of modes to retain the desired explained variance ratio
num_modes = num_modes_90_percent

# Reduce dimensions of U, S, and Vt
U_reduced = U[:, :num_modes]
S_reduced = np.diag(S[:num_modes])
Vt_reduced = Vt[:num_modes, :]

# Check the shapes
print("Reduced U shape:", U_reduced.shape)
print("Reduced S shape:", S_reduced.shape)
print("Reduced Vt shape:", Vt_reduced.shape)

# Reconstruct the original data using the reduced matrices
# reconstructed_data = np.dot(U_reduced, np.dot(S_reduced, Vt_reduced))
# height, width = downsampled_images_gray[0].shape

# Reshape the reconstructed data to the original shape
# reconstructed_images = reconstructed_data.reshape((len(downsampled_images_gray), height, width))
reconstructed_images= U_reduced @ S_reduced @ Vt_reduced+ mean_image11
print(reconstructed_images.shape)
# Display the top 10 reconstructed images
import matplotlib.pyplot as plt

# Plot the top 10 reconstructed images
plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images[i].reshape(downsampled_images_gray[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

print(U_reduced)
print(S_reduced)
print(Vt_reduced)

"""**Question 4**

Adding Noise
"""

import cv2
import os
import shutil
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np

def add_gaussian_noise(image, magnitude):
    mean = 0
    std_dev = magnitude * 255 / 100  # Convert magnitude to standard deviation
    noisy_image = image.copy()
    cv2.randn(noisy_image, mean, std_dev)
    noisy_image = cv2.add(image, noisy_image)
    return np.clip(noisy_image, 0, 255).astype(np.uint8)

def frames_with_noise(video_path, interval_seconds, color_scale, noise_percentages):
    cap = cv2.VideoCapture(video_path)
    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(frame_rate * interval_seconds)
    frame_count = 0
    saved_frame_count = 0

    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == False:
            break
        frame_count += 1
        if frame_count % frame_interval == 0:
            saved_frame_count += 1
            for noise_percentage in noise_percentages:
                noisy_frame = add_gaussian_noise(frame, noise_percentage)
                folder_name = f"Noise_{noise_percentage}"
                os.makedirs(folder_name, exist_ok=True)
                frame_filename = os.path.join(folder_name, f"frame_{saved_frame_count:04d}.jpg")
                cv2.imwrite(frame_filename, noisy_frame)

    cap.release()

    print(f"Total frames with noise extracted: {saved_frame_count}")

# Example usage:
video_path = "/content/drive/MyDrive/3CYL_Flow11.mp4"
interval_seconds = 0.5
color_scale = 'RGB'
noise_percentages = [20, 40, 60, 80]  # Noise percentages to apply

frames_with_noise(video_path, interval_seconds, color_scale, noise_percentages)

import os
import cv2
import numpy as np

# Function to load images from a folder
def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        if os.path.isfile(img_path):
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale
            if img is not None:
                images.append(img)
    return images

# # Function to display sizes of images
# def display_images_sizes(images):
#     for i, img in enumerate(images):
#         print(f"Image {i+1}: Rows = {img.shape[0]}, Columns = {img.shape[1]}, No. of pixels = {img.size}")


noisy_images1 = load_images_from_folder("Noise_20")
noisy_images2= load_images_from_folder("Noise_40")
noisy_images3 = load_images_from_folder("Noise_60")
noisy_images4 = load_images_from_folder("Noise_80")

"""Downsample Provision"""

import os
import cv2

# Function to downsample images in a folder
def downsample_images(folder_path, new_width, new_height):
    downsampled_images = []
    for filename in os.listdir(folder_path):
        img_path = os.path.join(folder_path, filename)
        if os.path.isfile(img_path):
            # Read the image
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                # Downsample the image
                downsampled_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
                downsampled_images.append(downsampled_img)
    return downsampled_images

# Define the folder paths for noisy images
folder_paths = ["Noise_20", "Noise_40", "Noise_60", "Noise_80"]

# Specify the new dimensions for downsampling
new_width = 1558
new_height = 708

# List to store downsampled images for each noise level
downsampled_images_gray_20 = downsample_images("Noise_20", new_width, new_height)
downsampled_images_gray_40 = downsample_images("Noise_40", new_width, new_height)
downsampled_images_gray_60 = downsample_images("Noise_60", new_width, new_height)
downsampled_images_gray_80 = downsample_images("Noise_80", new_width, new_height)

# # Display shape of each downsampled image for each noise level
# print("Downsampled Images for Noise Level 20%:")
# for img in downsampled_images_gray_20:
#     print("Image shape:", img.shape)

# print("\nDownsampled Images for Noise Level 40%:")
# for img in downsampled_images_gray_40:
#     print("Image shape:", img.shape)

# print("\nDownsampled Images for Noise Level 60%:")
# for img in downsampled_images_gray_60:
#     print("Image shape:", img.shape)

# print("\nDownsampled Images for Noise Level 80%:")
# for img in downsampled_images_gray_80:
#     print("Image shape:", img.shape)

plt.imshow(downsampled_images_gray_20[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

plt.imshow(downsampled_images_gray_40[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

plt.imshow(downsampled_images_gray_60[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

plt.imshow(downsampled_images_gray_80[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

mean_image_20 = np.mean(downsampled_images_gray_20, axis=0)
mean_image_40 = np.mean(downsampled_images_gray_40, axis=0)
mean_image_60 = np.mean(downsampled_images_gray_60, axis=0)
mean_image_80 = np.mean(downsampled_images_gray_80, axis=0)

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

# Stack images into a matrix
stacked_images_20 = stack_images(downsampled_images_gray_20)
mean_image11_20 = np.mean(stacked_images_20, axis=0)
centered_images_20 = stacked_images_20 - mean_image11_20
data_matrix_20=centered_images_20

# Stack images into a matrix
stacked_images_40 = stack_images(downsampled_images_gray_40)
mean_image11_40 = np.mean(stacked_images_40, axis=0)
centered_images_40 = stacked_images_40 - mean_image11_40
data_matrix_40=centered_images_40

# Stack images into a matrix
stacked_images_60 = stack_images(downsampled_images_gray_60)
mean_image11_60 = np.mean(stacked_images_60, axis=0)
centered_images_60 = stacked_images_60 - mean_image11_60
data_matrix_60=centered_images_60

# Stack images into a matrix
stacked_images_80 = stack_images(downsampled_images_gray_80)
mean_image11_80 = np.mean(stacked_images_80, axis=0)
centered_images_80 = stacked_images_80 - mean_image11_80
data_matrix_80=centered_images_80

# Perform Singular Value Decomposition (SVD)
U_20, S_20, Vt_20 = np.linalg.svd(data_matrix_20, full_matrices=False)
print(U_20)
print("spatial modes ",U_20.shape)
print(S_20)
print("singular values ",S_20.shape)
print(Vt_20)
print("temporal coefficients ",Vt_20.shape)

U_40, S_40, Vt_40 = np.linalg.svd(data_matrix_40, full_matrices=False)
print(U_40)
print("spatial modes ",U_40.shape)
print(S_40)
print("singular values ",S_40.shape)
print(Vt_40)
print("temporal coefficients ",Vt_40.shape)

U_60, S_60, Vt_60 = np.linalg.svd(data_matrix_60, full_matrices=False)
print(U_60)
print("spatial modes ",U_60.shape)
print(S_60)
print("singular values ",S_60.shape)
print(Vt_60)
print("temporal coefficients ",Vt_60.shape)

U_80, S_80, Vt_80 = np.linalg.svd(data_matrix_80, full_matrices=False)
print(U_80)
print("spatial modes ",U_80.shape)
print(S_80)
print("singular values ",S_80.shape)
print(Vt_80)
print("temporal coefficients ",Vt_80.shape)

# # Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_20) + 1), S_20, linestyle='-', color="red")
plt.title('Top Eigenvalues 20% Noise')
plt.xlabel('Eigenvalue Index')
plt.ylabel('Eigenvalue')
plt.show()

# # Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_40) + 1), S_40, linestyle='-', color="red")
plt.title('Top Eigenvalues 40% Noise')
plt.xlabel('Eigenvalue Index')
plt.ylabel('Eigenvalue')
plt.show()

# # Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_60) + 1), S_60, linestyle='-', color="red")
plt.title('Top Eigenvalues 60% Noise')
plt.xlabel('Eigenvalue Index')
plt.ylabel('Eigenvalue')
plt.show()

# # Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_80) + 1), S_80, linestyle='-', color="red")
plt.title('Top Eigenvalues 80% Noise')
plt.xlabel('Eigenvalue Index')
plt.ylabel('Eigenvalue')
plt.show()

# Calculate total energy
total_energy_20 = np.sum(S_20 ** 2)

# Calculate energy contribution of each mode
energy_contributions_20 = (S_20 ** 2) / total_energy_20

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_20:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_20[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_20[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_20')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_40 = np.sum(S_40 ** 2)

# Calculate energy contribution of each mode
energy_contributions_40 = (S_40 ** 2) / total_energy_40

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_40:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_40[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_40[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_40')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_60 = np.sum(S_60 ** 2)

# Calculate energy contribution of each mode
energy_contributions_60 = (S_60 ** 2) / total_energy_60

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_60:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_60[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_60[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_60')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_80 = np.sum(S_80 ** 2)

# Calculate energy contribution of each mode
energy_contributions_80 = (S_80 ** 2) / total_energy_80

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_80:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_80[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_80[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_80')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_20 = np.sum(S_20 ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_20 = np.sum(S_20[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_20 = (total_energy_top_10_modes_20 / total_energy_20) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_20:", total_contribution_top_10_modes_percentage_20, "%")

# Calculate total energy
total_energy_40 = np.sum(S_40 ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_40 = np.sum(S_40[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_40 = (total_energy_top_10_modes_40 / total_energy_40) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_40:", total_contribution_top_10_modes_percentage_40, "%")

# Calculate total energy
total_energy_60 = np.sum(S_60 ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_60 = np.sum(S_60[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_60 = (total_energy_top_10_modes_60 / total_energy_60) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_60:", total_contribution_top_10_modes_percentage_60, "%")

# Calculate total energy
total_energy_80 = np.sum(S_80 ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_80 = np.sum(S_80[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_80 = (total_energy_top_10_modes_80 / total_energy_80) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_80:", total_contribution_top_10_modes_percentage_80, "%")

required_variance_20 = 0.95
# Calculate cumulative variance
cumulative_variance_20 = np.cumsum(S_20 ** 2) / np.sum(S_20 ** 2)

num_modes_90_percent_20 = np.argmax(cumulative_variance_20 >= required_variance_20) + 1

print(f"Number of modes to retain {required_variance_20 * 100}% variance: {num_modes_90_percent_20}")

required_variance_40 = 0.95
# Calculate cumulative variance
cumulative_variance_40 = np.cumsum(S_40 ** 2) / np.sum(S_40 ** 2)

num_modes_90_percent_40 = np.argmax(cumulative_variance_40 >= required_variance_40) + 1

print(f"Number of modes to retain {required_variance_40 * 100}% variance: {num_modes_90_percent_40}")

required_variance_60 = 0.95
# Calculate cumulative variance
cumulative_variance_60 = np.cumsum(S_60 ** 2) / np.sum(S_60 ** 2)

num_modes_90_percent_60 = np.argmax(cumulative_variance_60 >= required_variance_60) + 1

print(f"Number of modes to retain {required_variance_60 * 100}% variance: {num_modes_90_percent_60}")

required_variance_80 = 0.95
# Calculate cumulative variance
cumulative_variance_80 = np.cumsum(S_80 ** 2) / np.sum(S_80 ** 2)

num_modes_90_percent_80 = np.argmax(cumulative_variance_80 >= required_variance_80) + 1

print(f"Number of modes to retain {required_variance_80 * 100}% variance: {num_modes_90_percent_80}")

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_20) + 1), cumulative_variance_20, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance_20 = 0.95
num_modes_90_percent_20 = np.argmax(cumulative_variance_20 >= required_variance_20) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent_20, cumulative_variance_20[num_modes_90_percent_20 - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent_20, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent_20, color='blue', linestyle='--')
plt.show()

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_40) + 1), cumulative_variance_40, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance_40 = 0.95
num_modes_90_percent_40 = np.argmax(cumulative_variance_40 >= required_variance_40) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent_40, cumulative_variance_40[num_modes_90_percent_40 - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent_40, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent_40, color='blue', linestyle='--')
plt.show()

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_60) + 1), cumulative_variance_60, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance_60 = 0.95
num_modes_90_percent_60 = np.argmax(cumulative_variance_60 >= required_variance_60) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent_60, cumulative_variance_60[num_modes_90_percent_60 - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent_60, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent_60, color='blue', linestyle='--')
plt.show()

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_80) + 1), cumulative_variance_80, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance_80 = 0.95
num_modes_90_percent_80 = np.argmax(cumulative_variance_80 >= required_variance_80) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent_80, cumulative_variance_80[num_modes_90_percent_80 - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent_80, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent_80, color='blue', linestyle='--')
plt.show()

# Number of modes to retain the desired explained variance ratio
num_modes_20 = num_modes_90_percent_20

# Reduce dimensions of U, S, and Vt
U_reduced_20 = U_20[:, :num_modes_20]
S_reduced_20 = np.diag(S_20[:num_modes_20])
Vt_reduced_20 = Vt_20[:num_modes_20, :]

# Check the shapes
print("Reduced U shape:", U_reduced_20.shape)
print("Reduced S shape:", S_reduced_20.shape)
print("Reduced Vt shape:", Vt_reduced_20.shape)

# Number of modes to retain the desired explained variance ratio
num_modes_40 = num_modes_90_percent_40

# Reduce dimensions of U, S, and Vt
U_reduced_40 = U_40[:, :num_modes_40]
S_reduced_40 = np.diag(S_40[:num_modes_40])
Vt_reduced_40 = Vt_40[:num_modes_40, :]

# Check the shapes
print("Reduced U shape:", U_reduced_40.shape)
print("Reduced S shape:", S_reduced_40.shape)
print("Reduced Vt shape:", Vt_reduced_40.shape)

# Number of modes to retain the desired explained variance ratio
num_modes_60 = num_modes_90_percent_60

# Reduce dimensions of U, S, and Vt
U_reduced_60 = U_60[:, :num_modes_60]
S_reduced_60 = np.diag(S_60[:num_modes_60])
Vt_reduced_60 = Vt_60[:num_modes_60, :]

# Check the shapes
print("Reduced U shape:", U_reduced_60.shape)
print("Reduced S shape:", S_reduced_60.shape)
print("Reduced Vt shape:", Vt_reduced_60.shape)

# Number of modes to retain the desired explained variance ratio
num_modes_80 = num_modes_90_percent_80

# Reduce dimensions of U, S, and Vt
U_reduced_80 = U_80[:, :num_modes_80]
S_reduced_80 = np.diag(S_80[:num_modes_80])
Vt_reduced_80 = Vt_80[:num_modes_80, :]

# Check the shapes
print("Reduced U shape:", U_reduced_80.shape)
print("Reduced S shape:", S_reduced_80.shape)
print("Reduced Vt shape:", Vt_reduced_80.shape)

# Reconstruct the original data using the reduced matrices
# reconstructed_data_20 = np.dot(U_reduced_20, np.dot(S_reduced_20, Vt_reduced_20))
# height_20, width_20 = downsampled_images_gray[0].shape

# Reshape the reconstructed data to the original shape
# reconstructed_images_20 = reconstructed_data_20.reshape((len(downsampled_images_gray), height_20, width_20))
reconstructed_images_20 = U_reduced_20 @ S_reduced_20 @ Vt_reduced_20 + mean_image11_20
# print(reconstructed_images_20.shape)
# Display the top 10 reconstructed images
import matplotlib.pyplot as plt

# Plot the top 10 reconstructed images
plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_20[i].reshape(downsampled_images_gray_20[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Reconstruct the original data using the reduced matrices
# reconstructed_data_40 = np.dot(U_reduced_40, np.dot(S_reduced_40, Vt_reduced_40))
# height_40, width_40 = downsampled_images_gray[0].shape

# Reshape the reconstructed data to the original shape
# reconstructed_images_40 = reconstructed_data_40.reshape((len(downsampled_images_gray), height_40, width_40))
reconstructed_images_40 = U_reduced_40 @ S_reduced_40 @ Vt_reduced_40 + mean_image11_40
# print(reconstructed_images_40.shape)
# Display the top 10 reconstructed images
import matplotlib.pyplot as plt

# Plot the top 10 reconstructed images
plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_40[i].reshape(downsampled_images_gray_40[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Reconstruct the original data using the reduced matrices
# reconstructed_data_60 = np.dot(U_reduced_60, np.dot(S_reduced_60, Vt_reduced_60))
# height_60, width_60 = downsampled_images_gray[0].shape

# Reshape the reconstructed data to the original shape
# reconstructed_images_60 = reconstructed_data_60.reshape((len(downsampled_images_gray), height_60, width_60))
reconstructed_images_60 = U_reduced_60 @ S_reduced_60 @ Vt_reduced_60 + mean_image11_60
# print(reconstructed_images_60.shape)
# Display the top 10 reconstructed images
import matplotlib.pyplot as plt

# Plot the top 10 reconstructed images
plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_60[i].reshape(downsampled_images_gray_60[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Reconstruct the original data using the reduced matrices
# reconstructed_data_80 = np.dot(U_reduced_80, np.dot(S_reduced_80, Vt_reduced_80))
# height_80, width_80 = downsampled_images_gray[0].shape

# Reshape the reconstructed data to the original shape
# reconstructed_images_80 = reconstructed_data_80.reshape((len(downsampled_images_gray), height_80, width_80))
reconstructed_images_80 = U_reduced_80 @ S_reduced_80 @ Vt_reduced_80 + mean_image11_80
# print(reconstructed_images_80.shape)
# Display the top 10 reconstructed images
import matplotlib.pyplot as plt

# Plot the top 10 reconstructed images
plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_80[i].reshape(downsampled_images_gray_80[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

"""*#Noise-type:2*

Adding Salt and Pepper Noise
"""

import cv2
import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np

def add_salt_and_pepper_noise(image, salt_percent, pepper_percent):
    noisy_image = image.copy()
    height, width = noisy_image.shape[:2]
    salt_count = int(height * width * salt_percent / 100)
    pepper_count = int(height * width * pepper_percent / 100)

    # Add salt noise
    salt_coords = [np.random.randint(0, i - 1, salt_count) for i in noisy_image.shape[:2]]
    noisy_image[salt_coords[0], salt_coords[1], :] = 255

    # Add pepper noise
    pepper_coords = [np.random.randint(0, i - 1, pepper_count) for i in noisy_image.shape[:2]]
    noisy_image[pepper_coords[0], pepper_coords[1], :] = 0

    return noisy_image

def frames_with_noise(video_path, interval_seconds, color_scale, salt_percentage, pepper_percentages):
    cap = cv2.VideoCapture(video_path)
    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(frame_rate * interval_seconds)
    frame_count = 0
    saved_frame_count = 0

    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == False:
            break
        frame_count += 1
        if frame_count % frame_interval == 0:
            saved_frame_count += 1
            for pepper_percentage in pepper_percentages:
                noisy_frame = add_salt_and_pepper_noise(frame, salt_percentage, pepper_percentage)
                folder_name = f"Noise_{salt_percentage}_{pepper_percentage}_salt"
                os.makedirs(folder_name, exist_ok=True)
                frame_filename = os.path.join(folder_name, f"frame_{saved_frame_count:04d}.jpg")
                cv2.imwrite(frame_filename, noisy_frame)

    cap.release()

    print(f"Total frames with noise extracted: {saved_frame_count}")

# Example usage:
video_path = "/content/drive/MyDrive/3CYL_Flow11.mp4"
interval_seconds = 0.5
color_scale = 'RGB'
salt_percentage = 20  # Fixed salt noise percentage
pepper_percentages = [20, 40, 60, 80]  # Pepper noise percentages to apply

frames_with_noise(video_path, interval_seconds, color_scale, salt_percentage, pepper_percentages)

import os
import cv2
import numpy as np

# Function to load images from a folder
def load_images_from_folder_salt(folder):
    images = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        if os.path.isfile(img_path):
            img = cv2.imread(img_path)  # Load image in grayscale
            if img is not None:
                images.append(img)
    return images

# Load images from different folders
noisy_images_20_salt = load_images_from_folder_salt("Noise_20_20_salt")
noisy_images_40_salt = load_images_from_folder_salt("Noise_20_40_salt")
noisy_images_60_salt = load_images_from_folder_salt("Noise_20_60_salt")
noisy_images_80_salt = load_images_from_folder_salt("Noise_20_80_salt")

"""*Downsample provision*"""

import os
import cv2

# Function to downsample images in a folder
def downsample_images_1(folder_path, new_width, new_height):
    downsampled_images = []
    for filename in os.listdir(folder_path):
        img_path = os.path.join(folder_path, filename)
        if os.path.isfile(img_path):
            # Read the image
            img = cv2.imread(img_path)
            if img is not None:
                # Downsample the image
                downsampled_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
                downsampled_images.append(downsampled_img)
    return downsampled_images

# Define the folder paths for noisy images
folder_paths = ["Noise_20_40_salt", "Noise_20_40_salt", "Noise_20_60_salt", "Noise_20_80_salt"]

# Specify the new dimensions for downsampling
new_width = 800
new_height = 364

# List to store downsampled images for each noise level
downsampled_images_gray_20_salt = downsample_images_1("Noise_20_40_salt", new_width, new_height)
downsampled_images_gray_40_salt = downsample_images_1("Noise_20_40_salt", new_width, new_height)
downsampled_images_gray_60_salt = downsample_images_1("Noise_20_60_salt", new_width, new_height)
downsampled_images_gray_80_salt = downsample_images_1("Noise_20_80_salt", new_width, new_height)

# # Display shape of each downsampled image for each noise level
# print("Downsampled Images for Noise Level 20% with Salt Noise:")
# for img in downsampled_images_gray_20_salt:
#     print("Image shape:", img.shape)

# print("\nDownsampled Images for Noise Level 40% with Salt Noise:")
# for img in downsampled_images_gray_40_salt:
#     print("Image shape:", img.shape)

# print("\nDownsampled Images for Noise Level 60% with Salt Noise:")
# for img in downsampled_images_gray_60_salt:
#     print("Image shape:", img.shape)

# print("\nDownsampled Images for Noise Level 80% with Salt Noise:")
# for img in downsampled_images_gray_80_salt:
#     print("Image shape:", img.shape)

import cv2
import os
import shutil
import numpy as np
import matplotlib.pyplot as plt

plt.imshow(downsampled_images_gray_20_salt[0])  # Display the first downsampled image
plt.title('Image')
plt.axis('off')
plt.show()
noisy_images_20_salt[0].shape

# downsampled_images_gray_20_salt=noisy_images_20_salt
# downsampled_images_gray_40_salt=noisy_images_40_salt
# downsampled_images_gray_60_salt=noisy_images_60_salt
# downsampled_images_gray_80_salt=noisy_images_80_salt

plt.imshow(downsampled_images_gray_40_salt[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

plt.imshow(downsampled_images_gray_60_salt[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

plt.imshow(downsampled_images_gray_80_salt[0])  # Display the first downsampled image
plt.title('Downsampled Image')
plt.axis('off')
plt.show()

import numpy as np

# Calculate mean images for each noise level
mean_image_20_salt = np.mean(downsampled_images_gray_20_salt, axis=0)
mean_image_40_salt = np.mean(downsampled_images_gray_40_salt, axis=0)
mean_image_60_salt = np.mean(downsampled_images_gray_60_salt, axis=0)
mean_image_80_salt = np.mean(downsampled_images_gray_80_salt, axis=0)

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

# Stack images into a matrix
stacked_images_20_salt = stack_images(downsampled_images_gray_20_salt)
mean_image_20_salt = np.mean(stacked_images_20_salt, axis=0)
centered_images_20_salt = stacked_images_20_salt - mean_image_20_salt
data_matrix_20_salt = centered_images_20_salt

# Stack images into a matrix
stacked_images_40_salt = stack_images(downsampled_images_gray_40_salt)
mean_image_40_salt = np.mean(stacked_images_40_salt, axis=0)
centered_images_40_salt = stacked_images_40_salt - mean_image_40_salt
data_matrix_40_salt = centered_images_40_salt

# Stack images into a matrix
stacked_images_60_salt = stack_images(downsampled_images_gray_60_salt)
mean_image_60_salt = np.mean(stacked_images_60_salt, axis=0)
centered_images_60_salt = stacked_images_60_salt - mean_image_60_salt
data_matrix_60_salt = centered_images_60_salt

# Stack images into a matrix
stacked_images_80_salt = stack_images(downsampled_images_gray_80_salt)
mean_image_80_salt = np.mean(stacked_images_80_salt, axis=0)
centered_images_80_salt = stacked_images_80_salt - mean_image_80_salt
data_matrix_80_salt = centered_images_80_salt

# Perform Singular Value Decomposition (SVD)
U_20_salt, S_20_salt, Vt_20_salt = np.linalg.svd(data_matrix_20_salt, full_matrices=False)
print(U_20_salt)
print("spatial modes ", U_20_salt.shape)
print(S_20_salt)
print("singular values ", S_20_salt.shape)
print(Vt_20_salt)
print("temporal coefficients ", Vt_20_salt.shape)

# Perform Singular Value Decomposition (SVD)
U_40_salt, S_40_salt, Vt_40_salt = np.linalg.svd(data_matrix_40_salt, full_matrices=False)
print(U_40_salt)
print("spatial modes ", U_40_salt.shape)
print(S_40_salt)
print("singular values ", S_40_salt.shape)
print(Vt_40_salt)
print("temporal coefficients ", Vt_40_salt.shape)

# Perform Singular Value Decomposition (SVD)
U_60_salt, S_60_salt, Vt_60_salt = np.linalg.svd(data_matrix_60_salt, full_matrices=False)
print(U_60_salt)
print("spatial modes ", U_60_salt.shape)
print(S_60_salt)
print("singular values ", S_60_salt.shape)
print(Vt_60_salt)
print("temporal coefficients ", Vt_60_salt.shape)

# Perform Singular Value Decomposition (SVD)
U_80_salt, S_80_salt, Vt_80_salt = np.linalg.svd(data_matrix_80_salt, full_matrices=False)
print(U_80_salt)
print("spatial modes ", U_80_salt.shape)
print(S_80_salt)
print("singular values ", S_80_salt.shape)
print(Vt_80_salt)
print("temporal coefficients ", Vt_80_salt.shape)

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_20_salt) + 1), S_20_salt, linestyle='-', color="red")
plt.title('Top Eigenvalues 20% Salt Noise')
plt.xlabel('Eigenvalue Index')
plt.grid(True)
plt.ylabel('Eigenvalue')
plt.show()

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_40_salt) + 1), S_40_salt, linestyle='-', color="red")
plt.title('Top Eigenvalues 40% Salt Noise')
plt.xlabel('Eigenvalue Index')
plt.ylabel('Eigenvalue')
plt.grid(True)
plt.show()

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_60_salt) + 1), S_60_salt, linestyle='-', color="red")
plt.title('Top Eigenvalues 60% Salt Noise')
plt.xlabel('Eigenvalue Index')
plt.ylabel('Eigenvalue')
plt.grid(True)

plt.show()

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_80_salt) + 1), S_80_salt, linestyle='-', color="red")
plt.title('Top Eigenvalues 80% Salt Noise')
plt.xlabel('Eigenvalue Index')
plt.grid(True)
plt.ylabel('Eigenvalue')
plt.show()

# Calculate total energy
total_energy_20_salt = np.sum(S_20_salt ** 2)

# Calculate energy contribution of each mode
energy_contributions_20_salt = (S_20_salt ** 2) / total_energy_20_salt

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_20_salt:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_20_salt[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_20_salt[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_20_salt')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_40_salt = np.sum(S_40_salt ** 2)

# Calculate energy contribution of each mode
energy_contributions_40_salt = (S_40_salt ** 2) / total_energy_40_salt

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_40_salt:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_40_salt[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_40_salt[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_40_salt')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_60_salt = np.sum(S_60_salt ** 2)

# Calculate energy contribution of each mode
energy_contributions_60_salt = (S_60_salt ** 2) / total_energy_60_salt

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_60_salt:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_60_salt[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_60_salt[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_60_salt')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_80_salt = np.sum(S_80_salt ** 2)

# Calculate energy contribution of each mode
energy_contributions_80_salt = (S_80_salt ** 2) / total_energy_80_salt

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes for S_80_salt:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions_80_salt[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions_80_salt[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes for S_80_salt')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
plt.show()

# Calculate total energy
total_energy_20_salt = np.sum(S_20_salt ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_20_salt = np.sum(S_20_salt[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_20_salt = (total_energy_top_10_modes_20_salt / total_energy_20_salt) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_20_salt:", total_contribution_top_10_modes_percentage_20_salt, "%")

# Calculate total energy
total_energy_40_salt = np.sum(S_40_salt ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_40_salt = np.sum(S_40_salt[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_40_salt = (total_energy_top_10_modes_40_salt / total_energy_40_salt) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_40_salt:", total_contribution_top_10_modes_percentage_40_salt, "%")

# Calculate total energy
total_energy_60_salt = np.sum(S_60_salt ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_60_salt = np.sum(S_60_salt[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_60_salt = (total_energy_top_10_modes_60_salt / total_energy_60_salt) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_60_salt:", total_contribution_top_10_modes_percentage_60_salt, "%")

# Calculate total energy
total_energy_80_salt = np.sum(S_80_salt ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes_80_salt = np.sum(S_80_salt[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage_80_salt = (total_energy_top_10_modes_80_salt / total_energy_80_salt) * 100

print("Total contribution of the top 10 modes as a percentage of total energy for S_80_salt:", total_contribution_top_10_modes_percentage_80_salt, "%")

required_variance_20_salt = 0.95
# Calculate cumulative variance
cumulative_variance_20_salt = np.cumsum(S_20_salt ** 2) / np.sum(S_20_salt ** 2)

num_modes_90_percent_20_salt = np.argmax(cumulative_variance_20_salt >= required_variance_20_salt) + 1

print(f"Number of modes to retain {required_variance_20_salt * 100}% variance: {num_modes_90_percent_20_salt}")

required_variance_40_salt = 0.95
# Calculate cumulative variance
cumulative_variance_40_salt = np.cumsum(S_40_salt ** 2) / np.sum(S_40_salt ** 2)

num_modes_90_percent_40_salt = np.argmax(cumulative_variance_40_salt >= required_variance_40_salt) + 1

print(f"Number of modes to retain {required_variance_40_salt * 100}% variance: {num_modes_90_percent_40_salt}")

required_variance_60_salt = 0.95
# Calculate cumulative variance
cumulative_variance_60_salt = np.cumsum(S_60_salt ** 2) / np.sum(S_60_salt ** 2)

num_modes_90_percent_60_salt = np.argmax(cumulative_variance_60_salt >= required_variance_60_salt) + 1

print(f"Number of modes to retain {required_variance_60_salt * 100}% variance: {num_modes_90_percent_60_salt}")

required_variance_80_salt = 0.95
# Calculate cumulative variance
cumulative_variance_80_salt = np.cumsum(S_80_salt ** 2) / np.sum(S_80_salt ** 2)

num_modes_90_percent_80_salt = np.argmax(cumulative_variance_80_salt >= required_variance_80_salt) + 1

print(f"Number of modes to retain {required_variance_80_salt * 100}% variance: {num_modes_90_percent_80_salt}")

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_20_salt) + 1), cumulative_variance_20_salt, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance_20_salt = 0.95
num_modes_90_percent_20_salt = np.argmax(cumulative_variance_20_salt >= required_variance_20_salt) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent_20_salt, cumulative_variance_20_salt[num_modes_90_percent_20_salt - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent_20_salt, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent_20_salt, color='blue', linestyle='--')
plt.show()

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_40_salt) + 1), cumulative_variance_40_salt, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 95% of the energy
required_variance_40_salt = 0.95
num_modes_95_percent_40_salt = np.argmax(cumulative_variance_40_salt >= required_variance_40_salt) + 1

# Plot a marker for 95% variance
plt.plot(num_modes_95_percent_40_salt, cumulative_variance_40_salt[num_modes_95_percent_40_salt - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_95_percent_40_salt, color='blue', linestyle='--')
plt.show()

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_60_salt) + 1), cumulative_variance_60_salt, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 95% of the energy
required_variance_60_salt = 0.95
num_modes_95_percent_60_salt = np.argmax(cumulative_variance_60_salt >= required_variance_60_salt) + 1

# Plot a marker for 95% variance
plt.plot(num_modes_95_percent_60_salt, cumulative_variance_60_salt[num_modes_95_percent_60_salt - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_95_percent_60_salt, color='blue', linestyle='--')
plt.show()

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S_80_salt) + 1), cumulative_variance_80_salt, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 95% of the energy
required_variance_80_salt = 0.95
num_modes_95_percent_80_salt = np.argmax(cumulative_variance_80_salt >= required_variance_80_salt) + 1

# Plot a marker for 95% variance
plt.plot(num_modes_95_percent_80_salt, cumulative_variance_80_salt[num_modes_95_percent_80_salt - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_95_percent_80_salt, color='blue', linestyle='--')
plt.show()

# Number of modes to retain the desired explained variance ratio
num_modes_20_salt = num_modes_90_percent_20_salt

# Reduce dimensions of U, S, and Vt
U_reduced_20_salt = U_20_salt[:, :num_modes_20_salt]
S_reduced_20_salt = np.diag(S_20_salt[:num_modes_20_salt])
Vt_reduced_20_salt = Vt_20_salt[:num_modes_20_salt, :]

# Check the shapes
print("Reduced U shape:", U_reduced_20_salt.shape)
print("Reduced S shape:", S_reduced_20_salt.shape)
print("Reduced Vt shape:", Vt_reduced_20_salt.shape)

# Number of modes to retain the desired explained variance ratio
num_modes_40_salt = num_modes_90_percent_40_salt

# Reduce dimensions of U, S, and Vt
U_reduced_40_salt = U_40_salt[:, :num_modes_40_salt]
S_reduced_40_salt = np.diag(S_40_salt[:num_modes_40_salt])
Vt_reduced_40_salt = Vt_40_salt[:num_modes_40_salt, :]

# Check the shapes
print("Reduced U shape:", U_reduced_40_salt.shape)
print("Reduced S shape:", S_reduced_40_salt.shape)
print("Reduced Vt shape:", Vt_reduced_40_salt.shape)

# Number of modes to retain the desired explained variance ratio
num_modes_60_salt = num_modes_90_percent_60_salt

# Reduce dimensions of U, S, and Vt
U_reduced_60_salt = U_60_salt[:, :num_modes_60_salt]
S_reduced_60_salt = np.diag(S_60_salt[:num_modes_60_salt])
Vt_reduced_60_salt = Vt_60_salt[:num_modes_60_salt, :]

# Check the shapes
print("Reduced U shape:", U_reduced_60_salt.shape)
print("Reduced S shape:", S_reduced_60_salt.shape)
print("Reduced Vt shape:", Vt_reduced_60_salt.shape)

# Number of modes to retain the desired explained variance ratio
num_modes_80_salt = num_modes_90_percent_80_salt

# Reduce dimensions of U, S, and Vt
U_reduced_80_salt = U_80_salt[:, :num_modes_80_salt]
S_reduced_80_salt = np.diag(S_80_salt[:num_modes_80_salt])
Vt_reduced_80_salt = Vt_80_salt[:num_modes_80_salt, :]

# Check the shapes
print("Reduced U shape:", U_reduced_80_salt.shape)
print("Reduced S shape:", S_reduced_80_salt.shape)
print("Reduced Vt shape:", Vt_reduced_80_salt.shape)

# Clip the pixel values to the valid range
reconstructed_images_20_salt = U_reduced_20_salt @ S_reduced_20_salt @ Vt_reduced_20_salt
reconstructed_images_20_salt_clipped = np.clip(reconstructed_images_20_salt, 0, 1)

# Display the top 10 reconstructed images
import matplotlib.pyplot as plt

# Plot the top 10 reconstructed images
plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_20_salt_clipped[i].reshape(downsampled_images_gray_20_salt[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Reconstruct the original data using the reduced matrices for salt 40
reconstructed_images_40_salt = U_reduced_40_salt @ S_reduced_40_salt @ Vt_reduced_40_salt
reconstructed_images_40_salt_clipped = np.clip(reconstructed_images_40_salt, 0, 1)

# Display the top 10 reconstructed images for salt 40
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_40_salt_clipped[i].reshape(downsampled_images_gray_40_salt[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Reconstruct the original data using the reduced matrices for salt 60
reconstructed_images_60_salt = U_reduced_60_salt @ S_reduced_60_salt @ Vt_reduced_60_salt
reconstructed_images_60_salt_clipped = np.clip(reconstructed_images_60_salt, 0, 1)

# Display the top 10 reconstructed images for salt 60
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_60_salt_clipped[i].reshape(downsampled_images_gray_60_salt[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Reconstruct the original data using the reduced matrices for salt 80
reconstructed_images_80_salt = U_reduced_80_salt @ S_reduced_80_salt @ Vt_reduced_80_salt
reconstructed_images_80_salt_clipped = np.clip(reconstructed_images_80_salt, 0, 1)

# Display the top 10 reconstructed images for salt 80
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(reconstructed_images_80_salt_clipped[i].reshape(downsampled_images_gray_80_salt[0].shape), cmap=None)
    plt.title(f"Top Mode {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

"""**Question 5**"""

import cv2
import os
import shutil
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

def add_gaussian_noise(image, magnitude):
    mean = 0
    std_dev = magnitude * 255 / 100
    noisy_image = image.copy()
    cv2.randn(noisy_image, mean, std_dev)
    noisy_image = cv2.add(image, noisy_image)
    return np.clip(noisy_image, 0, 255).astype(np.uint8)

def frames_with_noise(video_path, interval_seconds, color_scale, noise_percentages):
    cap = cv2.VideoCapture(video_path)
    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(frame_rate * interval_seconds)
    frame_count = 0
    saved_frame_count = 0

    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == False:
            break
        frame_count += 1
        if frame_count % frame_interval == 0:
            saved_frame_count += 1
            for noise_percentage in noise_percentages:
                noisy_frame = add_gaussian_noise(frame, noise_percentage)
                folder_name = f"Denoise_{noise_percentage}"  # Change folder name here
                os.makedirs(folder_name, exist_ok=True)
                frame_filename = os.path.join(folder_name, f"frame_{saved_frame_count:04d}.jpg")
                cv2.imwrite(frame_filename, noisy_frame)

    cap.release()

    print(f"Total frames with noise extracted: {saved_frame_count}")

# Example usage:
video_path = "/content/drive/MyDrive/3CYL_Flow11.mp4"
interval_seconds = 0.5
color_scale = 'RGB'
noise_percentages = [20, 40, 60, 80]  # Noise percentages to apply

frames_with_noise(video_path, interval_seconds, color_scale, noise_percentages)

# Function to load images from a folder
def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        if os.path.isfile(img_path):
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale
            if img is not None:
                images.append(img)
    return images

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

"""*Denoise using Gaussian Blur*

*For 20% blur*
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Function to apply Gaussian blur denoising to images
def denoise_with_gaussian_blur(images, kernel_size=(5, 5), sigma_x=0):
    denoised_images = []
    for img in images:
        denoised_img = cv2.GaussianBlur(img, kernel_size, sigma_x)
        denoised_images.append(denoised_img)
    return denoised_images

import random
noisy_images_20 = load_images_from_folder("Denoise_20")
denoised_images_20 = denoise_with_gaussian_blur(noisy_images_20)
num_images = len(noisy_images_20)
random_indices = random.sample(range(num_images), 5)

for i in random_indices:
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(noisy_images_20[i])
    plt.title('Noisy Image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(denoised_images_20[i])
    plt.title('Denoised Image')
    plt.axis('off')

    plt.show()

denoised_images_20[0].shape

len(denoised_images_20)

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

# Stack images into a matrix
stacked_images = stack_images(denoised_images_20)
print(stacked_images)
stacked_images.shape

# Compute mean image
mean_image11 = np.mean(stacked_images, axis=0)
print(mean_image11)
mean_image11.shape

# Step 2: Subtract the mean from each image
centered_images = stacked_images - mean_image11
print(centered_images)
print(centered_images.shape)

data_matrix=centered_images

# Perform Singular Value Decomposition (SVD)
U, S, Vt = np.linalg.svd(data_matrix, full_matrices=False)
print(U)
print("spatial modes ",U.shape)
print(S)
print("singular values ",S.shape)
print(Vt)
print("temporal coefficients ",Vt.shape)

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), S, linestyle='-', color="red")
plt.title('Singular Values Distribution')
plt.xlabel('Index')
plt.ylabel('Singular Value')
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate energy contribution of each mode
energy_contributions = (S ** 2) / total_energy

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
# plt.grid(True)
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes = np.sum(S[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage = (total_energy_top_10_modes / total_energy) * 100

print("Total contribution of the top 10 modes as a percentage of total energy:", total_contribution_top_10_modes_percentage,"%")

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
# Calculate cumulative variance
cumulative_variance = np.cumsum(S ** 2) / np.sum(S ** 2)

num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

print(f"Number of modes to retain {required_variance * 100}% variance: {num_modes_90_percent}")

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), cumulative_variance, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent, cumulative_variance[num_modes_90_percent - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent, color='blue', linestyle='--')
plt.show()

"""*For 80% Blur*

---



---


"""

import random
noisy_images_80 = load_images_from_folder("Denoise_80")
denoised_images_80 = denoise_with_gaussian_blur(noisy_images_80)
num_images = len(noisy_images_80)
random_indices = random.sample(range(num_images), 5)

for i in random_indices:
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(noisy_images_80[i])
    plt.title('Noisy Image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(denoised_images_80[i])
    plt.title('Denoised Image')
    plt.axis('off')

    plt.show()

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

# Stack images into a matrix
stacked_images = stack_images(denoised_images_80)
print(stacked_images)
stacked_images.shape

# Compute mean image
mean_image11 = np.mean(stacked_images, axis=0)
print(mean_image11)
mean_image11.shape

# Step 2: Subtract the mean from each image
centered_images = stacked_images - mean_image11
print(centered_images)
print(centered_images.shape)

data_matrix=centered_images

# Perform Singular Value Decomposition (SVD)
U, S, Vt = np.linalg.svd(data_matrix, full_matrices=False)
print(U)
print("spatial modes ",U.shape)
print(S)
print("singular values ",S.shape)
print(Vt)
print("temporal coefficients ",Vt.shape)

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), S, linestyle='-', color="red")
plt.title('Singular Values Distribution')
plt.xlabel('Index')
plt.ylabel('Singular Value')
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate energy contribution of each mode
energy_contributions = (S ** 2) / total_energy

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
# plt.grid(True)
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes = np.sum(S[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage = (total_energy_top_10_modes / total_energy) * 100

print("Total contribution of the top 10 modes as a percentage of total energy:", total_contribution_top_10_modes_percentage,"%")

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
# Calculate cumulative variance
cumulative_variance = np.cumsum(S ** 2) / np.sum(S ** 2)

num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

print(f"Number of modes to retain {required_variance * 100}% variance: {num_modes_90_percent}")

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), cumulative_variance, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent, cumulative_variance[num_modes_90_percent - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent, color='blue', linestyle='--')
plt.show()

"""*Denoise using NLM*

*For 20% Noise*
"""

import cv2

# Function to apply Non-local Means Denoising to images
def denoise_with_nlm(images, h=10, templateWindowSize=7, searchWindowSize=21):
    denoised_images = []
    for img in images:
        denoised_img = cv2.fastNlMeansDenoising(img, None, h, templateWindowSize, searchWindowSize)
        denoised_images.append(denoised_img)
    return denoised_images

# Loading noisy images from folder
noisy_images = load_images_from_folder("Denoise_20")

# Applying NLM denoising
denoised_images_nlm = denoise_with_nlm(noisy_images)

# Display original and denoised images
num_images = len(noisy_images)
for i in range(5):
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(noisy_images[i])
    plt.title('Noisy Image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(denoised_images_nlm[i])
    plt.title('Denoised Image (NLM)')
    plt.axis('off')

    plt.show()

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

# Stack images into a matrix
stacked_images = stack_images(denoised_images_nlm)
print(stacked_images)
stacked_images.shape

# Compute mean image
mean_image11 = np.mean(stacked_images, axis=0)
print(mean_image11)
mean_image11.shape

# Step 2: Subtract the mean from each image
centered_images = stacked_images - mean_image11
print(centered_images)
print(centered_images.shape)

data_matrix=centered_images

# Perform Singular Value Decomposition (SVD)
U, S, Vt = np.linalg.svd(data_matrix, full_matrices=False)
print(U)
print("spatial modes ",U.shape)
print(S)
print("singular values ",S.shape)
print(Vt)
print("temporal coefficients ",Vt.shape)

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), S, linestyle='-', color="red")
plt.title('Singular Values Distribution')
plt.xlabel('Index')
plt.ylabel('Singular Value')
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate energy contribution of each mode
energy_contributions = (S ** 2) / total_energy

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
# plt.grid(True)
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes = np.sum(S[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage = (total_energy_top_10_modes / total_energy) * 100

print("Total contribution of the top 10 modes as a percentage of total energy:", total_contribution_top_10_modes_percentage,"%")

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
# Calculate cumulative variance
cumulative_variance = np.cumsum(S ** 2) / np.sum(S ** 2)

num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

print(f"Number of modes to retain {required_variance * 100}% variance: {num_modes_90_percent}")

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), cumulative_variance, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent, cumulative_variance[num_modes_90_percent - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent, color='blue', linestyle='--')
plt.show()

"""*For 80% Noise*"""

# Loading noisy images from folder
noisy_images1 = load_images_from_folder("Denoise_80")

# Applying NLM denoising
denoised_images_nlm1 = denoise_with_nlm(noisy_images1)

# Display original and denoised images
num_images = len(noisy_images1)
for i in range(5):
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(noisy_images1[i])
    plt.title('Noisy Image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(denoised_images_nlm1[i])
    plt.title('Denoised Image (NLM)')
    plt.axis('off')

    plt.show()

# Function to flatten and stack images into a matrix
def stack_images(images):
    stacked_images = np.array([image.flatten() for image in images])
    return stacked_images

# Stack images into a matrix
stacked_images1 = stack_images(denoised_images_nlm1)
print(stacked_images1)
stacked_images1.shape

# Compute mean image
mean_image12 = np.mean(stacked_images1, axis=0)
print(mean_image11)
mean_image12.shape

# Step 2: Subtract the mean from each image
centered_images1= stacked_images1 - mean_image12
print(centered_images1)
print(centered_images1.shape)

data_matrix1=centered_images1

# Perform Singular Value Decomposition (SVD)
U, S, Vt = np.linalg.svd(data_matrix1, full_matrices=False)
print(U)
print("spatial modes ",U.shape)
print(S)
print("singular values ",S.shape)
print(Vt)
print("temporal coefficients ",Vt.shape)

# Plot the top eigenvalues
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), S, linestyle='-', color="red")
plt.title('Singular Values Distribution')
plt.xlabel('Index')
plt.ylabel('Singular Value')
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate energy contribution of each mode
energy_contributions = (S ** 2) / total_energy

# Display energy contribution of the top 10 modes
print("Energy contribution of the top 10 modes:")
for i in range(10):
    print(f"Mode {i+1}: {energy_contributions[i] * 100:.2f}%")

# Plot energy contribution of the top 10 modes
plt.figure(figsize=(8, 6))
plt.bar(np.arange(1, 11), energy_contributions[:10] * 100, color='blue')
plt.title('Energy Contribution of Top 10 Modes')
plt.xlabel('Mode')
plt.ylabel('Energy Contribution (%)')
plt.xticks(np.arange(1, 11))
# plt.grid(True)
plt.show()

# Calculate total energy
total_energy = np.sum(S ** 2)

# Calculate total energy contributed by the top 10 modes
total_energy_top_10_modes = np.sum(S[:10] ** 2)

# Calculate total contribution of the top 10 modes as a percentage of total energy
total_contribution_top_10_modes_percentage = (total_energy_top_10_modes / total_energy) * 100

print("Total contribution of the top 10 modes as a percentage of total energy:", total_contribution_top_10_modes_percentage,"%")

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
# Calculate cumulative variance
cumulative_variance = np.cumsum(S ** 2) / np.sum(S ** 2)

num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

print(f"Number of modes to retain {required_variance * 100}% variance: {num_modes_90_percent}")

# Plot the cumulative variance
plt.figure(figsize=(8, 6))
plt.plot(np.arange(1, len(S) + 1), cumulative_variance, linestyle='-', color="red")
plt.title('Cumulative Variance vs No. of Modes')
plt.xlabel('Modes')
plt.ylabel('Cumulative Variance')

# Find the number of modes required to retain 90% of the energy
required_variance = 0.95
num_modes_90_percent = np.argmax(cumulative_variance >= required_variance) + 1

# Plot a marker for 90% variance
plt.plot(num_modes_90_percent, cumulative_variance[num_modes_90_percent - 1], marker="o", markersize=10, markerfacecolor="green")
plt.axvline(x=num_modes_90_percent, color='blue', linestyle='--')
# plt.axhline(x=num_modes_90_percent, color='blue', linestyle='--')
plt.show()

"""*To explore, we tried CNN. But the no of comparisons it would make was not possible to be computed by the systems at hand. Hence, we have completed our analysis with 2 methods.*"""

